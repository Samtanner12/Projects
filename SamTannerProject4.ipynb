{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"e9bGhbYm1yff"},"source":["\n","\n","this project tries to predict stars from imdb reviews. It is on a 10 star scale 6 and up is positive 5 or less is negative sentiment. I will aslo try to preduict the number of stars\n","step number1  is to load the  data. There are 50,000 total samples, 25,000 training samples and 25,000 testing samples. inside traing and testing  there are 12,500 positive reviews and 12,500 negative reviews.\n","I load the files and split the lines into a vector containing both the number of stars acheived and the and the word content of the review. The file is in LIBSVM format which means that the data is stored in a form of  \"word\" : \"frequency\". the word corresponds to an index in the imbd.vocab file, the frequency is number of times that specific word appears in the review.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1650335329648,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"KZwdsS_v0pPF"},"outputs":[],"source":["#opening and reading the data\n","i = open('Trainlabeled.feat', 'r') \n","j = open('labeledBow.feat', 'r')\n","training = [line.split(' ') for line in i.readlines()]\n","testing = [line.split(' ') for line in j.readlines()]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p70j6trP1zYQ"},"source":[" Since  \"word\" is split from \"frequency\" with a colon I created a 2 dimensional array with the first index being the word number and the second the frequency This was then moved into a key value dictionary. i then put the dictionary and the review target was placed into another dictionary with two keys. The first key the number of starts the review gave to the movie. the second key si the word frequency pairs. these dictionarys are then put into an array to finish the preprocessing.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7679,"status":"ok","timestamp":1650335337324,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"p6VmkoKb16gf"},"outputs":[],"source":["#data arrays\n","trainData = []\n","testData = []\n","# loop over all of the training features and testing features\n","for train_feature, test_feature in zip(training, testing):\n","  # split the frequency\n","  trainIN = [f.split(':') for f in train_feature[1:]] \n","  testIN = [f.split(':') for f in test_feature[1:]]\n","  trainINDict = {}\n","  testInDict = {}\n","  # create key val pairs in the new dict\n","\n","  for i in trainIN:\n","      trainINDict[int(i[0].strip('\\n'))] = int(i[1].strip('\\n'))\n","\n","  for j in testIN:\n","\n","    testInDict[int(j[0].strip('\\n'))] = int(j[1].strip('\\n'))\n","  # put the input and output data into the new dict\n","  trainDict2 = {'output': int(train_feature[0]), 'input': trainINDict}\n","  testDict2 = {'output': int(test_feature[0]), 'input': testInDict}\n","  # add dicts to the arrays\n","  trainData.append(trainDict2)\n","  testData.append(testDict2)\n","  # open the file\n","imdbEr = open('imdbEr.txt', 'r')\n","\n","# read the value line by line\n","wordVals = [float(v.strip('\\n')) for v in imdbEr.readlines()]"]},{"cell_type":"markdown","metadata":{"id":"nblk2djN1_FI"},"source":["The first models to train will try to predict the exact number of stars as outputs.The sklearn models that are being implemented are LogisticRegression, MLPClassifier, and MLPRegressor. I will reduce each input down to a single value which corresponsds to the sum of all word polarities in the review. This will use just the word occurence, not frequency."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1382,"status":"ok","timestamp":1650335338704,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"akuVJVXf1_lb"},"outputs":[],"source":["# calculuate polarity values for each review \n","X_train_occurrence = []\n","X_test_occurrence = []\n","\n","for i, test in zip(trainData, testData):\n","  sumTrain = 0\n","  sumTest = 0\n","  for word in i['input'].keys():\n","    sumTrain += wordVals[word]\n","  X_train_occurrence.append(sumTrain)\n","  for word in test['input'].keys():\n","    sumTest += wordVals[word]\n","  X_test_occurrence.append(sumTest)\n","\n","\n","  # collect targets into single array\n","y_train_multi = [train['output'] for train in trainData]\n","y_test_multi = [test['output'] for test in testData]\n","\n","# reshape input arrays\n","import numpy as np\n","X_train_occurrence_np = np.array(X_train_occurrence).reshape(-1, 1)\n","X_test_occurrence_np = np.array(X_test_occurrence).reshape(-1, 1)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":17542,"status":"ok","timestamp":1650335356244,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"Yt8-xIMO2GtU","outputId":"f4925fa5-62ab-4925-e58f-418c2a70702a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of multi class prediction using word occurrence\n","Logistic Regression accuracy: 0.36648\n","MLP Classifier accuracy: 0.366\n","MLP Regressor accuracy: 0.5701906969305168\n"]}],"source":["# create the models and test accuracy using occurence\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","\n","logistic_multi_occurrence = LogisticRegression()\n","logistic_multi_occurrence.fit(X_train_occurrence_np, y_train_multi)\n","logistic_multi_occurrence_accuracy = logistic_multi_occurrence.score(X_test_occurrence_np, y_test_multi)\n","\n","mlp_multi_occurrence = MLPClassifier()\n","mlp_multi_occurrence.fit(X_train_occurrence_np, y_train_multi)\n","mlp_multi_occurrence_accuracy = mlp_multi_occurrence.score(X_test_occurrence_np, y_test_multi)\n","\n","mlpr_multi_occurrence = MLPRegressor()\n","mlpr_multi_occurrence.fit(X_train_occurrence_np, y_train_multi)\n","mlpr_multi_occurrence_accuracy = mlpr_multi_occurrence.score(X_test_occurrence_np, y_test_multi)\n","\n","print(\"Accuracy of multi class prediction using word occurrence\")\n","print(f\"Logistic Regression accuracy: {logistic_multi_occurrence_accuracy}\")\n","print(f\"MLP Classifier accuracy: {mlp_multi_occurrence_accuracy}\")\n","print(f\"MLP Regressor accuracy: {mlpr_multi_occurrence_accuracy}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Fjcev4sc2Ij2"},"source":["By employing multi-class output and word occurrence, it is apparent that the MLP Regressor provides the most accurate predictions with an accuracy rate of 57%. Considering that there were 10 potential classes and the model was correct over 50% of the time, this is not a terrible outcome. The other two models had almost identical accuracy rates, both being correct around one-third of the time. Given that a random guess would result in a 10% accuracy rate, anything above 10% is considered learning.\n","\n","The next set of models will entail multi-class prediction utilizing word frequency as well. I plan to reduce the inputs once again and, this time, multiply the polarity value by the frequency of that word in the review. However, given that word occurrence is usually more crucial than frequency in the text, I do not expect this approach to dramatically enhance the predictions."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2020,"status":"ok","timestamp":1650335358262,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"VPWZJ1z62JDB"},"outputs":[],"source":["# calculuate polarity values for each review - this time incorporating word frequency\n","XTrainFreq = []\n","XTestFreq = []\n","\n","for i, test in zip(trainData, testData):\n","  sumTrain = 0\n","  sumTest = 0\n","  for word in i['input'].keys():\n","    sumTrain += (wordVals[word] * i['input'][word])\n","  XTrainFreq.append(sumTrain)\n","  for word in test['input'].keys():\n","    sumTest += (wordVals[word] * test['input'][word])\n","  XTestFreq.append(sumTest)\n","\n","\n","  # reshape input arrays again\n","TrainFreqNP = np.array(XTrainFreq).reshape(-1, 1)\n","TestFreqNP = np.array(XTestFreq).reshape(-1, 1)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":13230,"status":"ok","timestamp":1650335371490,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"gP-mopLm2M4T","outputId":"a6e260b1-ac14-4d38-9f3a-5a4738460d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of word frequency\n","Logistic Regression accuracy: 0.35984\n","MLP Classifier accuracy: 0.3606\n","MLP Regressor accuracy: 0.5358764186343842\n"]}],"source":["# create the models and test accuracy using frequency\n","logMultiFreq = LogisticRegression()\n","logMultiFreq.fit(TrainFreqNP, y_train_multi)\n","logMultiFreqAcc = logMultiFreq.score(TestFreqNP, y_test_multi)\n","mlpMultiFreq = MLPClassifier()\n","mlpMultiFreq.fit(TrainFreqNP, y_train_multi)\n","mlpMultiFreqAcc = mlpMultiFreq.score(TestFreqNP, y_test_multi)\n","mlprMultiFreq = MLPRegressor()\n","mlprMultiFreq.fit(TrainFreqNP, y_train_multi)\n","mlprMultiFreqAcc = mlprMultiFreq.score(TestFreqNP, y_test_multi)\n","print(\"Accuracy of word frequency\")\n","print(f\"Logistic Regression accuracy: {logMultiFreqAcc}\")\n","print(f\"MLP Classifier accuracy: {mlpMultiFreqAcc}\")\n","print(f\"MLP Regressor accuracy: {mlprMultiFreqAcc}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PAhLTaJH2O4W"},"source":["The outcomes are in line with expectations, showing similarity to the previous results. The MLP Regressor had the highest accuracy, while the other two models achieved comparable accuracies. Additionally, the results were somewhat inferior, providing evidence that word occurrence is more potent than word frequency. For the final models to be trained using this reduction method, I will be using binary classes (positive/negative) and the polarity values based solely on word occurrence, given its superior performance."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650335371490,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"d4jHwm822PeS"},"outputs":[],"source":["# collect targets into single array - using binary values\n","yTranBi = [1 if train['output'] > 5 else 0 for train in trainData]\n","yTestBi = [1 if test['output'] > 5 else 0 for test in testData]"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":6580,"status":"ok","timestamp":1650335378060,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"GRAbaqbi2RmI","outputId":"c73e5a9e-75a5-4f55-bd45-0ae5fe236d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of  word occurrence\n","Logistic Regression accuracy: 0.8498\n","MLP Classifier accuracy: 0.85096\n","MLP Regressor accuracy: 0.5513295157570759\n"]}],"source":["# create the models and test accuracy using occurence and binary outputs\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","logBiOCC = LogisticRegression()\n","logBiOCC.fit(X_train_occurrence_np, yTranBi)\n","logBiOccAcc = logBiOCC.score(X_test_occurrence_np, yTestBi)\n","mlpBIOcc = MLPClassifier()\n","mlpBIOcc.fit(X_train_occurrence_np, yTranBi)\n","mlpBiOccAcc = mlpBIOcc.score(X_test_occurrence_np, yTestBi)\n","mlprBiOcc = MLPRegressor()\n","mlprBiOcc.fit(X_train_occurrence_np, yTranBi)\n","mlprBiOccAcc = mlprBiOcc.score(X_test_occurrence_np, yTestBi)\n","print(\"Accuracy of  word occurrence\")\n","print(f\"Logistic Regression accuracy: {logBiOccAcc}\")\n","print(f\"MLP Classifier accuracy: {mlpBiOccAcc}\")\n","print(f\"MLP Regressor accuracy: {mlprBiOccAcc}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UK817Ayg2gQi"},"source":["Changing to a binary output resulted in a significant improvement in prediction accuracy. Both the Logistic Regression and MLP Classifier now have an accuracy of around 85%. Interestingly, the MLP Regressor, which was the top predictor for multi-class output, is the poorest predictor for binary class outputs. This indicates that predicting binary outputs is easier and more successful. The optimal results have been achieved by combining binary outputs with using word occurrence instead of frequency."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Qd7cMrVq2g-s"},"source":["# The Bag of Words\n","\n","My plan for the next stage of the project involves using the bag of words technique, where the input comprises large feature vectors, with each vector dimension corresponding to a different word in the review dictionary. However, there are around 90,000 words and 50,000 samples, which would result in 4.5 billion numbers if each sample had an input vector with 90,000 dimensions. Considering each number is a double, this would amount to 36 billion bytes of data, which is too much memory. Therefore, I decided to use minibatch training, which involves training on small batches of the total input to use less memory while still training the model on all the data. I opted for a minibatch size of 1000 inputs, where each iteration of partial fitting sees only 1000 inputs, and the model will see every training sample over 25 iterations. I plan to train two models: the first one is an SGDClassifier, which behaves like logistic regression during partial fitting when the argument loss='log' is specified, and the second is an MLPClassifier, which performed the best when using the reduction method."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650335378061,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"Ey6-1H3w2huX"},"outputs":[],"source":["# define minibatch size and number of iterations\n","minibachSize = 1000\n","totalSize = np.ceil(len(trainData))\n","iterations = int(totalSize / minibachSize)\n","# create SGDClassifier\n","from sklearn.linear_model import SGDClassifier\n","clf = SGDClassifier(loss='log')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZSBXA8sW2loV"},"source":["For the input vectors, I will solely rely on the word occurrence in the review, disregarding word frequency. The feature vector will contain the polarity value in that particular dimension only if the word appears in the review. If it does not appear, the feature vector will contain zero."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\samta\\OneDrive\\Desktop\\Racket\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Iteration 1/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 2/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 3/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 4/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 5/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 6/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 7/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 8/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 9/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 10/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 11/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 12/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 13/25: training accuracy - 0.846, validation accuracy - 0.846\n","Iteration 14/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 15/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 16/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 17/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 18/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 19/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 20/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 21/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 22/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 23/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 24/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 25/25: training accuracy - 1.0, validation accuracy - 1.0\n"]}],"source":["# train over number of iterations\n","for i in range(iterations): \n","  xTrainMini = []\n","  # grab training targets\n","  yTrainMini = yTranBi[i * minibachSize:(i + 1) * minibachSize]\n","  xTestMini = []\n","  # grab testing targets\n","  yTestMini = yTestBi[i * minibachSize:(i + 1) * minibachSize]\n","  # generate feature vectors\n","  for train, test in zip(trainData[i * minibachSize:(i + 1) * minibachSize], testData[i * minibachSize:(i + 1) * minibachSize]):\n","    trainIN = train['input']\n","    trainVec = []\n","    testIN = train['input']\n","    testVec = []\n","    # look at all words and check if they exist in review words\n","    # if the word exists append the polarity value\n","    # if not, append 0\n","    for word in range(len(wordVals)):\n","      if word in trainIN:\n","        trainVec.append(wordVals[word])\n","      else:\n","        trainVec.append(0)\n","      if word in testIN:\n","        testVec.append(wordVals[word])\n","      else:\n","        testVec.append(0)\n","    # add vectors to the minibatch\n","    xTrainMini.append(trainVec)\n","    xTestMini.append(testVec)\n","  # test the fit after minibatches are created\n","  clf.partial_fit(xTrainMini, yTrainMini, [0, 1])\n","  valAcc = clf.score(xTestMini, yTestMini)\n","  trainAcc = clf.score(xTrainMini, yTrainMini)\n","  print(f\"Iteration {i + 1}/{iterations}: training accuracy - {trainAcc}, validation accuracy - {valAcc}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1650336888123,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"FB6CMVV82pHo"},"outputs":[],"source":["# create MLPClassifier - using 1000 neurons in hidden layer\n","\n","from sklearn.neural_network import MLPClassifier\n","mlp = MLPClassifier(1000)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 2/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 3/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 4/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 5/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 6/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 7/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 8/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 9/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 10/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 11/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 12/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 13/25: training accuracy - 0.5, validation accuracy - 0.5\n","Iteration 14/25: training accuracy - 0.0, validation accuracy - 0.0\n","Iteration 15/25: training accuracy - 0.0, validation accuracy - 0.0\n","Iteration 16/25: training accuracy - 0.0, validation accuracy - 0.0\n","Iteration 17/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 18/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 19/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 20/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 21/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 22/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 23/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 24/25: training accuracy - 1.0, validation accuracy - 1.0\n","Iteration 25/25: training accuracy - 1.0, validation accuracy - 1.0\n"]}],"source":["for i in range(iterations):\n","  xTrainMinibatch = []\n","  yTrainMini = yTranBi[i * minibachSize:(i + 1) * minibachSize]\n","  xTestMini = []\n","  yTestMinibatch = yTestBi[i * minibachSize:(i + 1) * minibachSize]\n","  for train, test in zip(trainData[i * minibachSize:(i + 1) * minibachSize], testData[i * minibachSize:(i + 1) * minibachSize]):\n","    trainIN = train['input']\n","    trainVec = []\n","    testIN = train['input']\n","    test_vector = []\n","    for word in range(len(wordVals)):\n","      if word in trainIN:\n","        trainVec.append(wordVals[word])\n","      else:\n","        trainVec.append(0)\n","      if word in testIN:\n","        test_vector.append(wordVals[word])\n","      else:\n","        test_vector.append(0)\n","    xTrainMinibatch.append(trainVec)\n","    xTestMini.append(test_vector)\n","  mlp.partial_fit(xTrainMinibatch, yTrainMini, [0, 1])\n","  valAcc = mlp.score(xTestMini, yTestMinibatch)\n","  trainAcc = mlp.score(xTrainMinibatch, yTrainMini)\n","  print(f\"Iteration {i + 1}/{iterations}: training accuracy - {trainAcc}, validation accuracy - {valAcc}\")\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1650339895416,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"7Xq5xSnn28_R"},"outputs":[],"source":["import sklearn\n","train_data_shuffled = sklearn.utils.shuffle(trainData)\n","test_data_shuffled = sklearn.utils.shuffle(testData)\n","# collect shuffled targets\n","y_train_shuffled_binary = [1 if train['output'] > 5 else 0 for train in train_data_shuffled]\n","y_test_shuffled_binary = [1 if test['output'] > 5 else 0 for test in test_data_shuffled]"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1524957,"status":"ok","timestamp":1650341421659,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"BLDQxJXi3A6p","outputId":"a03c0a12-ba9d-4892-edeb-2121d3774c2c"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\samta\\OneDrive\\Desktop\\Racket\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n","  warnings.warn(\n"]},{"ename":"TypeError","evalue":"unsupported operand type(s) for +: 'dict' and 'int'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\samta\\OneDrive\\Desktop\\Racket\\SamTannerProject4.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/samta/OneDrive/Desktop/Racket/SamTannerProject4.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m valAcc \u001b[39m=\u001b[39m clf_shuffled\u001b[39m.\u001b[39mscore(xTestMini, yTestMinibatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/samta/OneDrive/Desktop/Racket/SamTannerProject4.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m trainAcc \u001b[39m=\u001b[39m clf_shuffled\u001b[39m.\u001b[39mscore(xTrainMinibatch, yTrainMini)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/samta/OneDrive/Desktop/Racket/SamTannerProject4.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m: training accuracy - \u001b[39m\u001b[39m{\u001b[39;00mtrainAcc\u001b[39m}\u001b[39;00m\u001b[39m, validation accuracy - \u001b[39m\u001b[39m{\u001b[39;00mvalAcc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'int'"]}],"source":["# logistic regression learning with shuffled inputs\n","clf_shuffled = SGDClassifier(loss='log')\n","for i in range(iterations): \n","  xTrainMinibatch = []\n","  yTrainMini = y_train_shuffled_binary[i * minibachSize:(i + 1) * minibachSize]\n","  xTestMini = []\n","  yTestMinibatch = y_test_shuffled_binary[i * minibachSize:(i + 1) * minibachSize]\n","  for i, test in zip(train_data_shuffled[i * minibachSize:(i + 1) * minibachSize], test_data_shuffled[i * minibachSize:(i + 1) * minibachSize]):\n","    trainIN = i['input']\n","    trainVec = []\n","    testIN = i['input']\n","    testVec = []\n","    for word in range(len(wordVals)):\n","      if word in trainIN:\n","        trainVec.append(wordVals[word])\n","      else:\n","        trainVec.append(0)\n","      if word in testIN:\n","        testVec.append(wordVals[word])\n","      else:\n","        testVec.append(0)\n","    # add vectors to the minibatch\n","    xTrainMinibatch.append(trainVec)\n","    xTestMini.append(testVec)\n","  # test the fit after minibatches are created\n","  clf_shuffled.partial_fit(xTrainMinibatch, yTrainMini, [0, 1])\n","  valAcc = clf_shuffled.score(xTestMini, yTestMinibatch)\n","  trainAcc = clf_shuffled.score(xTrainMinibatch, yTrainMini)\n","  print(f\"Iteration {i + 1}/{iterations}: training accuracy - {trainAcc}, validation accuracy - {valAcc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":760600,"status":"ok","timestamp":1650343733787,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"7DkROLxV3DZW","outputId":"e83a1ee6-6f8e-4f8d-b0bc-ab5c83fcc543"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1/25: training accuracy - 0.959, validation accuracy - 0.477\n","Iteration 2/25: training accuracy - 0.952, validation accuracy - 0.521\n","Iteration 3/25: training accuracy - 0.935, validation accuracy - 0.472\n","Iteration 4/25: training accuracy - 0.936, validation accuracy - 0.53\n","Iteration 5/25: training accuracy - 0.933, validation accuracy - 0.495\n","Iteration 6/25: training accuracy - 0.946, validation accuracy - 0.467\n","Iteration 7/25: training accuracy - 0.937, validation accuracy - 0.499\n","Iteration 8/25: training accuracy - 0.945, validation accuracy - 0.508\n","Iteration 9/25: training accuracy - 0.948, validation accuracy - 0.543\n","Iteration 10/25: training accuracy - 0.935, validation accuracy - 0.535\n","Iteration 11/25: training accuracy - 0.927, validation accuracy - 0.529\n","Iteration 12/25: training accuracy - 0.93, validation accuracy - 0.49\n","Iteration 13/25: training accuracy - 0.941, validation accuracy - 0.492\n","Iteration 14/25: training accuracy - 0.945, validation accuracy - 0.491\n","Iteration 15/25: training accuracy - 0.939, validation accuracy - 0.505\n","Iteration 16/25: training accuracy - 0.922, validation accuracy - 0.492\n","Iteration 17/25: training accuracy - 0.954, validation accuracy - 0.527\n","Iteration 18/25: training accuracy - 0.945, validation accuracy - 0.489\n","Iteration 19/25: training accuracy - 0.944, validation accuracy - 0.484\n","Iteration 20/25: training accuracy - 0.944, validation accuracy - 0.484\n","Iteration 21/25: training accuracy - 0.931, validation accuracy - 0.513\n","Iteration 22/25: training accuracy - 0.925, validation accuracy - 0.514\n","Iteration 23/25: training accuracy - 0.938, validation accuracy - 0.522\n","Iteration 24/25: training accuracy - 0.936, validation accuracy - 0.474\n","Iteration 25/25: training accuracy - 0.925, validation accuracy - 0.487\n"]}],"source":["# mlp learning with shuffled inputs\n","mlp_shuffled = MLPClassifier(1000)\n","\n","for i in range(iterations):\n","  xTrainMinibatch = []\n","  yTrainMini = y_train_shuffled_binary[i * minibachSize:(i + 1) * minibachSize]\n","\n","  xTestMini = []\n","  yTestMinibatch = y_test_shuffled_binary[i * minibachSize:(i + 1) * minibachSize]\n","  \n","  for i, test in zip(train_data_shuffled[i * minibachSize:(i + 1) * minibachSize], test_data_shuffled[i * minibachSize:(i + 1) * minibachSize]):\n","    trainIN = i['input']\n","    trainVec = []\n","    testIN = i['input']\n","    testVec = []\n","\n","    for word in range(len(wordVals)):\n","      if word in trainIN:\n","        trainVec.append(wordVals[word])\n","      else:\n","        trainVec.append(0)\n","      \n","      if word in testIN:\n","        testVec.append(wordVals[word])\n","      else:\n","        testVec.append(0)\n","\n","    xTrainMinibatch.append(trainVec)\n","    xTestMini.append(testVec)\n","\n","  mlp_shuffled.partial_fit(xTrainMinibatch, yTrainMini, [0, 1])\n","  valAcc = mlp_shuffled.score(xTestMini, yTestMinibatch)\n","  trainAcc = mlp_shuffled.score(xTrainMinibatch, yTrainMini)\n","  print(f\"Iteration {i + 1}/{iterations}: training accuracy - {trainAcc}, validation accuracy - {valAcc}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T1LN1n3K3F1E"},"source":["My accuraccy steadied around 50% for both the lmp and regression\n","\n","\n","The last thing I am going to try in this project is to do bag-of-words, but only on a certain number of the most common words.\n","\n","\n","The first step is to rank each word by totaling the number of times that word appeared in the reviews. I will only be looking at occurencde in each review of a word, not total number of times used. For words like \"and\", \"the\", and \"but\" this will reduce their total popularity.\n","\n","\n","Since only the training inputs are used to fit the model, I am only going to look at the words in the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650343733788,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"2CTDwHCd3Gzd"},"outputs":[],"source":["def get_n_popular_words(n):\n","  words = {}\n","  for word in range(len(wordVals)):\n","    words[word] = 0\n","  # count the number of word occurences\n","  for train in trainData:\n","    inputs = train['input']\n","    for word in inputs:\n","      words[word] += 1\n","  sortedWords = sorted(words.items(), key=lambda item: item[1])\n","  nSortedWords = sortedWords[-n:]\n","  mostPopular = [p[0] for p in nSortedWords]\n","  return mostPopular"]},{"cell_type":"markdown","metadata":{"id":"floF5PlF3Iom"},"source":["Now I can use feature vectors of any size, and hopefully fun the code faster to train more models. I am going to begin with the 1000 most popular words."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":502196,"status":"ok","timestamp":1650344235982,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"7N4JrA7g3Iwm","outputId":"368440c8-5eeb-49e4-f275-68b99ecaf715"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 1000 most popular words\n","Logistic Regression accuracy - 0.85864\n","MLPClassifier accuracy - 0.85444\n"]}],"source":["def create_vector(input, most_popular):\n","  vector = []\n","  for word in most_popular:\n","    if word in input:\n","      vector.append(wordVals[word])\n","    else:\n","      vector.append(0)\n","  return vector\n","# logistic regression and mlp learning with 1000 most popular words\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","n = 1000 # number of popular words\n","# creating mlp\n","mlp1000MostPop = MLPClassifier(1000)\n","#creating logistic regression\n","lr1000MostPop = LogisticRegression(max_iter=200)\n","XTrain1000 = []\n","Xtest1000 = []\n","mostPop1000 = get_n_popular_words(n) # get most popular words\n","# generat vectors\n","for train, test in zip(trainData, testData):\n","  XTrain1000.append(create_vector(train['input'], mostPop1000))\n","  Xtest1000.append(create_vector(test['input'], mostPop1000))\n","lr1000MostPop.fit(XTrain1000, yTranBi)\n","lr1000MostPopAcc = lr1000MostPop.score(Xtest1000, yTestBi)\n","# training mlp\n","mlp1000MostPop.fit(XTrain1000, yTranBi)\n","mlp1000MostPopAcc = mlp1000MostPop.score(Xtest1000, yTestBi)\n","print(\"1000 most popular words\")\n","print(f\"Logistic Regression accuracy - {lr1000MostPopAcc}\")\n","print(f\"MLPClassifier accuracy - {mlp1000MostPopAcc}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9ib3S7n73P2A"},"source":["Opting for the n most popular words leads to a significant improvement. By only utilizing the top 1000 most popular words, the accuracy of both the Logistic Regression and MLP Classifier models increased to approximately 85%. This is a noteworthy improvement, and these models are significantly more accurate than the prior bag-of-words models.\n","\n","For the ultimate models, I plan to employ the same model types (Logistic Regression and MLP Classifier) while utilizing the top 5000 most popular words."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1204714,"status":"ok","timestamp":1650345659306,"user":{"displayName":"Ethan","userId":"18408690860839396067"},"user_tz":240},"id":"zHN0evYd3P-R","outputId":"0bf5638c-b6e7-4dec-f29d-357a00652d6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 5000 most popular words\n","Logistic Regression accuracy - 0.87464\n","MLPClassifier accuracy - 0.87416\n"]}],"source":["# logistic regression and mlp learning with 5000 most popular words\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","n = 5000 # number of popular words\n","# creating mlp\n","mlp5000MostPop = MLPClassifier(1000)\n","#creating logistic regression\n","lr5000MostPop = LogisticRegression(max_iter=200)\n","xTrain5000 = []\n","xTest5000 = []\n","mostPop5000 = get_n_popular_words(n) # get most popular words\n","#  vectors\n","for train, test in zip(trainData, testData):\n","  xTrain5000.append(create_vector(train['input'], mostPop5000))\n","  xTest5000.append(create_vector(test['input'], mostPop5000))\n","lr5000MostPop.fit(xTrain5000, yTranBi)\n","lr5000MostPopAcc = lr5000MostPop.score(xTest5000, yTestBi)\n","# mlp\n","mlp5000MostPop.fit(xTrain5000, yTranBi)\n","mlp5000MostPopAcc = mlp5000MostPop.score(xTest5000, yTestBi)\n","print(\"5000 most popular words\")\n","print(f\"Logistic Regression accuracy - {lr5000MostPopAcc}\")\n","print(f\"MLPClassifier accuracy - {mlp5000MostPopAcc}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-nNb41WaBdkD"},"source":["the models were quite sucessful. The initial bag-of-words models trained through minibatch iterations were still not practical for efficient training and there remains room for further improvement. The models that utilized vector reduction also had success but had problems. These models revealed that word occurrence is superior to word frequency. predicting sentiment overall is simpler than predicting the number of stars provided in a rating.\n","\n","The models were able to improve accuracy by increasing the number of popular words from 1000 to 5000. The MLP model took longer to train than the logistic regression model, but both models achieved similar results."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyODaRUqe+166a5VZtvgUOJc","collapsed_sections":[],"name":"Project4.ipynb","provenance":[]},"kernelspec":{"display_name":"Racket","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"50196961ad2927027f66bea154cd55a8a17688cbd6b9e031fd82c1b7086be43a"}}},"nbformat":4,"nbformat_minor":0}
